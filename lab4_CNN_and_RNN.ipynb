{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4 CNN and RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOct78Ps6GX7uSnZ7G0Afrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ecd1664babdc47498a1c9303fa57c206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df6c3dffc9d0457a90f84b0e3161ffad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a7ddcc871c6e4d18b1cda40a253aa35b",
              "IPY_MODEL_d005bed916d247b6936411deda06aa59"
            ]
          }
        },
        "df6c3dffc9d0457a90f84b0e3161ffad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7ddcc871c6e4d18b1cda40a253aa35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c178d0e936b240bd846804dc97073b8e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e33104989874b0e9ae505b46d46c556"
          }
        },
        "d005bed916d247b6936411deda06aa59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da5c1634b4c74abd8d96bab90299e759",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [08:46&lt;00:00, 324075.81it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff4bd38f181040d8b3619b29550b4fbe"
          }
        },
        "c178d0e936b240bd846804dc97073b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e33104989874b0e9ae505b46d46c556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da5c1634b4c74abd8d96bab90299e759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff4bd38f181040d8b3619b29550b4fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkbu-kennycheng/comp3057/blob/main/lab4_CNN_and_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYh987Rg4woA"
      },
      "source": [
        "# Image classification with CNN\n",
        "\n",
        "We are going to train a CNN model for image classification using `PyTorch`. Let's import `torch` first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECBGWg53GMZx"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgR-UXWU5BhQ"
      },
      "source": [
        "## [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset\n",
        "\n",
        "With `CIFAR10` dataset, we could train a model to classify airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. It serve as testing purpose for machine model development.\n",
        "\n",
        "![](https://url2img-web.herokuapp.com/aHR0cHM6Ly93d3cuY3MudG9yb250by5lZHUvfmtyaXovY2lmYXIuaHRtbA==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvCDeucK5KUM"
      },
      "source": [
        "### Loading CIFAR10 dataset with torchvision\n",
        "\n",
        "`CIFAR10` could be downloaded with `torchvision` module directly by `torchvision.datasets`. We also need `torchvision.transforms` and transform data to [`Tensor`](https://pytorch.org/docs/stable/tensors.html). Tensor is a multi-dimensional container for data of different types.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oPgsAOR5PfP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "ecd1664babdc47498a1c9303fa57c206",
            "df6c3dffc9d0457a90f84b0e3161ffad",
            "a7ddcc871c6e4d18b1cda40a253aa35b",
            "d005bed916d247b6936411deda06aa59",
            "c178d0e936b240bd846804dc97073b8e",
            "6e33104989874b0e9ae505b46d46c556",
            "da5c1634b4c74abd8d96bab90299e759",
            "ff4bd38f181040d8b3619b29550b4fbe"
          ]
        },
        "outputId": "79e32b8c-1404-4258-ee52-68c01137c8df"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "train_set = datasets.CIFAR10(\"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = datasets.CIFAR10(\"./data\", download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecd1664babdc47498a1c9303fa57c206",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml0i9wWrGwXF"
      },
      "source": [
        "### Split train, valid and test data\n",
        "\n",
        "We the wrap it with `DataLoader`, which helps us to shuffle and load data in each iteration during the train process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnPBlEAzGqny"
      },
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True)\n",
        "test_loader = DataLoader(test_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDahS3zOmc0c"
      },
      "source": [
        "The labels are represented using `0 - 9`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_Imzf7LBkp"
      },
      "source": [
        "labels = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YsAFZMs5Okn"
      },
      "source": [
        "## Building a CNN model with `Sequential` API\n",
        "\n",
        "Let's take a look to CNN model architecture that we are going to build.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/mnist.png)\n",
        "\n",
        "`PyTorch` provides us API in `torch.nn.Conv2d` for applying convolution.\n",
        "\n",
        "![](https://url2img-web.herokuapp.com/aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9nZW5lcmF0ZWQvdG9yY2gubm4uQ29udjJkLmh0bWw=)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfO0q8OC1lhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8ba5f7-98de-4bad-ea08-cc248524a5aa"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "  nn.Sequential(\n",
        "    nn.Conv2d(3,6,5),     # Convolutions between INPUT and C1\n",
        "    nn.ReLU(),            # Activation function of Convolutions, which outputs C1\n",
        "    nn.MaxPool2d((2,2)),  # 2D max pooling subsampling between C1 and S2\n",
        "    nn.Conv2d(6, 16, 5),  # Convolutions between S2 and C3\n",
        "    nn.ReLU(),            # Activation function of Convolutions, whcih outputs C3\n",
        "    nn.MaxPool2d(2),      # 2D max pooling subsampling between C3 and S4\n",
        "  ),\n",
        "  nn.Flatten(),          # Flatten reformats input data from 2d array to 1d array\n",
        "  nn.Sequential(\n",
        "    nn.Linear(16 * 5 * 5, 120), # F5\n",
        "    nn.ReLU(),            # activation of F5\n",
        "    nn.Linear(120, 84),   # F6\n",
        "    nn.ReLU(),            # activation of F6\n",
        "    nn.Linear(84, 10)     # OUTPUT\n",
        "  ),\n",
        ")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (2): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaSHX6ZiMl_I"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Loss function measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "\n",
        "We would use [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in our example CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F6ZQ5lgMyfv"
      },
      "source": [
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXl0pG7CM7Ab"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "This is how the model is updated based on the data it sees and its loss function.\n",
        "\n",
        "[Adam](https://arxiv.org/abs/1412.6980) optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. \n",
        "\n",
        "We set the learning rate to `0.001` in following configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8--garyM2jY"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syz-9zf5NRyI"
      },
      "source": [
        "## The training loop\n",
        "\n",
        "We first need to define how many `epoch` in the training process. An epoch means one complete pass of the training dataset through the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxUYwyHkNRbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0287b61-8442-4143-b95d-a0d4f7db6768"
      },
      "source": [
        "from tqdm import tqdm # for making progress bar\n",
        "\n",
        "NUM_EPOCHS = 1 # 1 epoch take a few minutes using Colab CPU Standard runtime\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loop = tqdm(train_loader, position=0, leave=True)\n",
        "\n",
        "  model.train() # put model in training mode\n",
        "  for (input, label) in loop:\n",
        "    optimizer.zero_grad()\n",
        "    output = model.forward(input)\n",
        "    loss = loss_function(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch [0/1]: 100%|██████████| 50000/50000 [07:41<00:00, 108.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMtW6MUTGaAu"
      },
      "source": [
        "## Test trained model\n",
        "\n",
        "In order to use the model for prediction, we need to put it in evaluation mode first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKjstkD8GOy_",
        "outputId": "015c308c-d1b6-42f8-d175-fb59b823a16f"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "loop = tqdm(test_loader, position=0, leave=True)\n",
        "model.eval() # put model in evaluation mode\n",
        "for (input, label) in loop:\n",
        "  output = model.forward(input)\n",
        "  _, predicted = torch.max(output.data, 1)\n",
        "  total += label.size(0)\n",
        "  correct += (predicted == label).sum().item()\n",
        "  loop.set_postfix(acc=(100*correct/total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:52<00:00, 189.51it/s, acc=45.9]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA8wz-Wo33B"
      },
      "source": [
        "# Text classification with RNN build from scratch\n",
        "\n",
        "This is a NLP task, which uses movie review form IMDB to classify it as positive review or negative review.\n",
        "\n",
        "![](https://url2img-web.herokuapp.com/aHR0cHM6Ly9haS5zdGFuZm9yZC5lZHUvfmFtYWFzL2RhdGEvc2VudGltZW50Lw==)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqYFksk6vsFZ",
        "outputId": "2747fd8d-7414-4539-d905-0caa72c09370"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La4cK2WJ5Nrz"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "`torchtext` provides us convenient API for downloading the dataset and split it in training set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwo7ud7T4eXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fef7b8e-85f3-4f4c-8436-4fd1ed4354ac"
      },
      "source": [
        "from torchtext.datasets import IMDB\n",
        "\n",
        "train_set, test_set = IMDB(split=('train', 'test'))\n",
        "\n",
        "train_label, train_data = zip(*train_set)\n",
        "test_label, test_data = zip(*test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 40.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFxu7kLwnEdv"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "Let's take a look to the data. Each review is written in english and paired with corresponding class `neg` or `pos`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSmOwyLghXA_",
        "outputId": "8c1da049-18a2-4c91-c5d0-4cb1be039c44"
      },
      "source": [
        "print(train_label[0])\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n",
            "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgzwL9igD-FM"
      },
      "source": [
        "## Preprocess text data\n",
        "\n",
        "Usually for NLP task, it require several necessary data preprocessing steps in order to transform data into appropriate input for the model. They are **standardization**, **tokenization** and **vectorization**.\n",
        "\n",
        "\n",
        "### Standardization\n",
        "\n",
        "You may notice that, the text in `train_data` contains `<br />` and captial characters. To Standardize it, we would replace `<br />` and punctuation as space and empty string. Finally transform all characters into lower case.\n",
        "\n",
        "For example, `<br />I rented I AM CURIOUS-YELLOW` becomes `i rented i am curious yellow`.\n",
        "\n",
        "### Tokenization\n",
        "\n",
        "After that, we would split input data into tokens by spaces. For example `i rented i am curious-yellow` becomes a list of words `[\"i\", \"rented\", \"i\", \"am\", \"curious\", \"yellow\"]`.\n",
        "\n",
        "### Numericalization and Vectorization\n",
        "\n",
        "Vectorization computes a vector for each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSZDIdt-ehLX"
      },
      "source": [
        "### Mapping text label to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmisO3N_ex1Y"
      },
      "source": [
        "# label_dict = {'neg': 0, 'pos': 1}\n",
        "label_dict = {'neg': torch.tensor([1.0, 0.0], dtype=torch.float), 'pos': torch.tensor([0.0, 1.0], dtype=torch.float)}\n",
        "train_label = [label_dict[label] for label in train_label]\n",
        "test_label = [label_dict[label] for label in test_label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUe5x9PBxtYZ"
      },
      "source": [
        "### Standardization and Tokenization\n",
        "\n",
        "We may use `custom_replace` from `torchtext.data.functional` together with regular expression to replace ``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9qPy66IKIMG"
      },
      "source": [
        "from torchtext.data.functional import custom_replace\n",
        "import re\n",
        "import string\n",
        "\n",
        "rules = [\n",
        "  (r'<br />', ' '),\n",
        "  (r'[%s]' % re.escape(string.punctuation), '')\n",
        "] + [(r'[%s]' % c, c.lower()) for c in string.ascii_uppercase]\n",
        "\n",
        "custom_replace_transform = custom_replace(rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9IpLoj6-C0"
      },
      "source": [
        "### Build a dictionary from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQj2NnqV4a0p",
        "outputId": "8df79096-dee5-466c-bfef-29325a9edb4c"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "vocab = build_vocab_from_iterator(train_data + test_data)\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfdaBatF7HGp"
      },
      "source": [
        "### Vectorize tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgN9EaWF7G2q"
      },
      "source": [
        "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
        "from itertools import takewhile\n",
        "\n",
        "train_data = numericalize_tokens_from_iterator(vocab, custom_replace_transform(train_data))\n",
        "test_data = numericalize_tokens_from_iterator(vocab, custom_replace_transform(test_data))\n",
        "\n",
        "tensorized_train_data = [torch.tensor([num for num in d], dtype=torch.long) for d in train_data]\n",
        "tensorized_test_data = [torch.tensor([num for num in d], dtype=torch.long) for d in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQCjL4hQvqU3",
        "outputId": "e8044b65-4927-47a0-a599-adc15338f887"
      },
      "source": [
        "print(len(tensorized_train_data[0]), tensorized_train_data[0])\n",
        "print(len(tensorized_test_data[0]), tensorized_test_data[0])\n",
        "print(train_label[:10])\n",
        "print(test_label[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1576 tensor([ 5,  0,  8,  ..., 10,  4,  2])\n",
            "1321 tensor([ 5,  0, 10,  ...,  3,  5,  7])\n",
            "[tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.])]\n",
            "[tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk6Da6DGhXNI"
      },
      "source": [
        "### Wrap with DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUjIv5dHhWrv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "tensorized_train_set = list(zip(tensorized_train_data, train_label))\n",
        "tensorized_test_set = list(zip(tensorized_test_data, test_label))\n",
        "\n",
        "# we only takes 10% samples from dataset, otherwise it will take very long time to train\n",
        "train_loader = DataLoader(tensorized_train_set, sampler=SubsetRandomSampler(range(0, len(tensorized_train_set), 10)))\n",
        "test_loader = DataLoader(tensorized_test_set, sampler=SubsetRandomSampler(range(0, len(tensorized_test_set), 10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mgM-RyJiCmD"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "\n",
        "### Overview\n",
        "\n",
        "```\n",
        "/--------\\          /--------\\         /--------\\         /--------\\\n",
        "| output |          | output |         | output |         | output |\n",
        "\\--------/          \\--------/         \\--------/         \\--------/\n",
        "     |                  |                  |                  |\n",
        " +-------+          +-------+          +-------+          +-------+\n",
        " |  RNN  |    =>    |  RNN  |    =>    |  RNN  |    =>    |  RNN  |    =>    ...\n",
        " +-------+  hidden  +-------+  hidden  +-------+  hidden  +-------+  hidden\n",
        "     |      state       |      state       |      state       |      state\n",
        "/--------\\          /--------\\         /--------\\         /--------\\\n",
        "|  input |          | input  |         |  input |         | input  |\n",
        "\\--------/          \\--------/         \\--------/         \\--------/\n",
        "    ^                   ^                  ^                  ^\n",
        "    i                 rented               i                  am\n",
        " (Vector)            (Vector)           (Vector)           (Vector)\n",
        "```\n",
        "\n",
        "### Recurrent Neural Network\n",
        "\n",
        "Here is an overview of our RNN. It's a simple recurrent feedforward neural network.\n",
        "\n",
        "```\n",
        "  +-------+\n",
        "  | input |\n",
        "  +-------+\n",
        "       |\n",
        "+-----------+    +--------+\n",
        "| embedding |    | hidden | <---O\n",
        "+-----------+    +--------+     |\n",
        "         \\       /              |\n",
        "       +----------+             |\n",
        "       | combined |             |\n",
        "       +----------+             |\n",
        "      /            \\            |\n",
        "      |        +---------+      |\n",
        "      |        | Sigmoid |      |\n",
        "      |        +---------+      |\n",
        "      |             |           |\n",
        "+---------+    +--------+       |\n",
        "| output  |    | hidden | ------O\n",
        "+---------+    +--------+\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhLNkYftiCGM"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_embeddings):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings, input_size)\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.acti = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden = self.initHidden()\n",
        "        for word_vector in self.embedding(input):\n",
        "          combined = torch.cat((word_vector.reshape(1, len(word_vector)), hidden), 1)\n",
        "          hidden = self.i2h(combined)\n",
        "          hidden = self.acti(hidden)\n",
        "          output = self.i2o(combined)\n",
        "        return output\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-h_fYGxQQI"
      },
      "source": [
        "n_hidden = 128      # size of hidden state\n",
        "embedding_dim = 512  # number of dimension for embedding vector\n",
        "\n",
        "model = RNN(embedding_dim, n_hidden, 2, len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb8e5Hm3RogD"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Loss function measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "\n",
        "We would use [`nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) in our example RNN model.\n",
        "\n",
        "![](https://url2img-web.herokuapp.com/aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9nZW5lcmF0ZWQvdG9yY2gubm4uQkNFV2l0aExvZ2l0c0xvc3MuaHRtbA==)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AvkG7Q8RogD"
      },
      "source": [
        "loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.ones([2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMM4bXz0T-UQ"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "`SDG` is a gradient descent (with momentum set to `0.9`) optimizer. For learning rate, we set to `0.001`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYagE8FkUBc6"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# Only SGD, SpareAdam and Adagrad support spare gradients\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjEX2PzDRTNo"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUcjZk_bRU-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7485bb9e-a06d-4834-be08-9f058b5ab5d0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "NUM_EPOCHS = 1 # 1 epoch take about 15 minutes using Colab CPU standard runtime\n",
        "               # 2 epochs give us 0.68 accuracy in evaluation\n",
        "\n",
        "model.train() # put model in training mode\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loop = tqdm(train_loader, position=0, leave=True)\n",
        "  \n",
        "  for (batch, labels) in loop:\n",
        "    for input in batch:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model.forward(input)\n",
        "\n",
        "      loss = loss_function(output, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [15:47<00:00,  2.64it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zNFrUSbbto"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMlT1VwobifW"
      },
      "source": [
        "model.eval() # put model in evaluation mode\n",
        "\n",
        "prediction_model = nn.Sequential(\n",
        "  model,\n",
        "  nn.LogSoftmax(dim=1),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xKh9KTGbhY9"
      },
      "source": [
        "### Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwNYh_CmFEDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4023a7c-21ad-43f2-8485-92e9230786cc"
      },
      "source": [
        "correct = 0\n",
        "loop = tqdm(test_loader, position=0, leave=True)\n",
        "\n",
        "for (batch, labels) in loop:\n",
        "  for input in batch:\n",
        "    output = prediction_model.forward(input)\n",
        "    truth = labels.argmax(0)[0]\n",
        "    guess = output.argmax(1)[0]\n",
        "    if truth == guess:\n",
        "      correct += 1\n",
        "print(f'Acc = {correct/len(loop)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [04:21<00:00,  9.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Acc = 0.5788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}